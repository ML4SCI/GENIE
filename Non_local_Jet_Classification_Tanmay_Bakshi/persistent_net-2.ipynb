{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Rx-zeYA_kCCQ",
    "outputId": "45ae448b-2fb4-4ab2-c2d1-7019e906613f"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZvb9rJ4kH2B",
    "outputId": "e20c9cda-1a5f-4a96-f9b9-24f036200f4c"
   },
   "source": [
    "! pip install uproot\n",
    "! pip install plotly\n",
    "! pip install \"notebook>=5.3\" \"ipywidgets>=7.5\"\n",
    "! pip install hdf5plugin\n",
    "! pip install awkward\n",
    "! pip install numba\n",
    "! pip install vector\n",
    "! pip install scikit-tda"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBaJg2SikkoS",
    "outputId": "57bc0721-1bb6-40f2-9f1b-ed177c7ab799"
   },
   "source": [
    "%ls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-x7GRRqdklD0",
    "outputId": "326f7b39-526b-48b8-a0cc-5ab33bfefba0"
   },
   "source": [
    "%cd drive/MyDrive/"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9zZqY5WXjww",
    "outputId": "ce385da7-3af9-498b-8e30-0feff76729d2"
   },
   "source": [
    "%cd reference_data/"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWQGjQftklUw",
    "outputId": "eb59d056-d886-46d4-abcb-af5bbe62d92a"
   },
   "source": [
    "%ls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUk_zfRWkljD"
   },
   "source": [
    "import h5py as hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "import awkward as ak\n",
    "import torch\n",
    "import tqdm.auto as tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fLu7KBikl5j"
   },
   "source": [
    "filename = 'train.h5'\n",
    "f = hp.File(filename, 'r')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYULPq2Zlqyd"
   },
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_jets_interactive(coordinates_array):\n",
    "    # Ensure the input is a NumPy array\n",
    "    if not isinstance(coordinates_array, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array.\")\n",
    "\n",
    "    # Extract the first 5 jets\n",
    "    jets = coordinates_array[:5]\n",
    "\n",
    "    # Initialize an empty list to hold individual figures\n",
    "    figs = []\n",
    "\n",
    "    for i, jet in enumerate(jets):\n",
    "        x, y, z, intensities = jet[..., 1], jet[..., 2], jet[..., 3], jet[..., 0]  # Using index 0 for intensity\n",
    "\n",
    "        # Create a scatter plot for each jet\n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "                color=intensities,  # set color to intensity\n",
    "                colorscale='Viridis',  # choose a colorscale\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            name=f'Jet {i+1}'\n",
    "        )])\n",
    "\n",
    "        # Update layout for better visibility\n",
    "        fig.update_layout(scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ))\n",
    "\n",
    "        # Add the figure to the list\n",
    "        figs.append(fig)\n",
    "\n",
    "    # Combine all figures into one with a dropdown menu\n",
    "    combined_fig = go.Figure()\n",
    "    for fig in figs:\n",
    "        combined_fig.add_trace(fig.data[0])  # Add the first (and only) trace from each figure\n",
    "\n",
    "    # Create a dropdown menu for selecting the jet\n",
    "    dropdown = dict(\n",
    "        active=0,\n",
    "        values=[f\"Jet {i+1}\" for i in range(len(figs))],\n",
    "        labels=[f\"Jet {i+1}\" for i in range(len(figs))]\n",
    "    )\n",
    "    combined_fig.update_layout(\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"dropdown\",\n",
    "                showactive=False,\n",
    "                buttons=list([\n",
    "                dict(label=jet_name, method=\"update\", args=[{\"visible\": [True if j == i else False for j in range(len(figs))]}])\n",
    "                    for i, jet_name in enumerate(dropdown[\"values\"])\n",
    "                ]),\n",
    "                pad={\"r\": 10, \"t\": 10},\n",
    "#                showactive=True,\n",
    "                x=0.1,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            )\n",
    "        ],\n",
    "        autosize=False,\n",
    "        width=500,\n",
    "        height=400,\n",
    "        margin=dict(l=50, r=50, b=100, t=100),\n",
    "        paper_bgcolor=\"LightSteelBlue\",\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show the combined plot with dropdown\n",
    "    combined_fig.show()\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzd50S2hlFyZ"
   },
   "source": [
    "import vector\n",
    "import numba as nb\n",
    "from vector.backends import awkward_constructors as awk\n",
    "from vector._compute import lorentz as lz\n",
    "\"\"\"\n",
    "        Takes a DataFrame and converts it into a Awkward array representation\n",
    "        with features relevant to our model.\n",
    "\n",
    "        :param df: Pandas DataFrame, The DataFrame with all the momenta-energy coordinates for all the particles\n",
    "        :param start: int, First element of the DataFrame\n",
    "        :param stop: int, Last element of the DataFrame\n",
    "        :return v: OrderedDict, A Ordered Dictionary with all properties of interest\n",
    "\n",
    "\n",
    "\n",
    "        Here the function is just computing 4 quantities of interest:\n",
    "        * Eta value relative to the jet\n",
    "        * Phi value relative to the jet\n",
    "        * Transverse Momentum of the Particle (log of it)\n",
    "        * Energy of the Particle (log of it)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Compute as many properties as required, keep scope for more always. Maybe use some of it to transform between latent spaces and some for Message Passing.\n",
    "# TODO: Initially, simply use the low-level features for everything and see how the model trained on low level features compares to model with domain knowledge.\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def compute_features(v, label, mom_objects):\n",
    "#     v['label'] = np.stack((label, 1-label), axis=-1)\n",
    "#     v['part_pt_log'] = np.log(jet_p4.pt())\n",
    "#     v['part_e_log'] = np.log(energy)\n",
    "#     v['part_etarel'] = vector._compute.spatial.deltaeta()\n",
    "#     v['part_phirel'] = jet_p4.pseudorapidity(jet_p4)\n",
    "\n",
    "\n",
    "def _transform(df, start=0, stop=-1):\n",
    "    from collections import OrderedDict\n",
    "    v = OrderedDict()\n",
    "\n",
    "    # generate the column list to be extracted\n",
    "    def _col_list(prefix, max_particles=200):\n",
    "        return ['%s_%d'%(prefix,i) for i in range(max_particles)]\n",
    "\n",
    "    df = df.iloc[start:stop]\n",
    "    # We take the values in the dataframe for all particles of a single event in each row\n",
    "    # px, py, pz, e are in separate arrays\n",
    "\n",
    "    _px = df[_col_list(prefix = 'PX')].values\n",
    "    _py = df[_col_list(prefix = 'PY')].values\n",
    "    _pz = df[_col_list(prefix = 'PZ')].values\n",
    "    _e = df[_col_list(prefix = 'E')].values\n",
    "    # # We filter out the non-0 non-negative energy particles\n",
    "    mask = _e > 0\n",
    "    n_particles = np.sum(mask, axis=1) # Number of particles for each event where energy is greater than 0\n",
    "\n",
    "\n",
    "    # # _p[mask] filters out the >0 energy particles, and flattens them, so that they can be recollected for each event from counts array.\n",
    "    px = ak.Array(_px[mask])\n",
    "    py = ak.Array(_py[mask])\n",
    "    pz = ak.Array(_pz[mask])\n",
    "    energy = ak.Array(_e[mask])\n",
    "    # # These are jagged arrays with each row for 1 event, and all particles in the row\n",
    "\n",
    "    # p4_lz = vector.array({\"x\": px, \"y\": py, \"z\": pz, \"t\": energy})\n",
    "        # Calculate jet parameters\n",
    "    jet_px = ak.sum(px, axis=0)\n",
    "    jet_py = ak.sum(py, axis=0)\n",
    "    jet_pz = ak.sum(pz, axis=0)\n",
    "    jet_energy = ak.sum(energy, axis=0)\n",
    "\n",
    "    jet_p4 = vector.obj(x = jet_px, y = jet_py, z = jet_pz, t = jet_energy)\n",
    "\n",
    "    # Transverse momentum (p_T)\n",
    "    jet_pt = np.sqrt(jet_px**2 + jet_py**2)\n",
    "    log_jet_pt = np.log(jet_pt)\n",
    "\n",
    "    # Energy\n",
    "    log_jet_energy = np.log(jet_energy)\n",
    "\n",
    "    # Pseudorapidity difference\n",
    "    jet_eta = 0.5 * np.log((np.sqrt(jet_px**2 + jet_py**2 + jet_pz**2) + jet_pz) / (np.sqrt(jet_px**2 + jet_py**2 + jet_pz**2) - jet_pz))\n",
    "    part_eta = 0.5 * np.log((np.sqrt(px**2 + py**2 + pz**2) + pz) / (np.sqrt(px**2 + py**2 + pz**2) - pz))\n",
    "    eta_rel = part_eta - jet_eta\n",
    "\n",
    "    # Storing the calculated parameters in the OrderedDict\n",
    "    v['jet_pt'] = jet_pt\n",
    "    v['jet_log_pt'] = log_jet_pt\n",
    "    v['jet_eta'] = jet_eta\n",
    "    v['part_ptrel'] = jet_pt/v['jet_pt']\n",
    "    v['log_jet_energy'] = log_jet_energy\n",
    "    v['eta_rel'] = eta_rel\n",
    "\n",
    "    # outputs\n",
    "    _label = df['is_signal_new'].values\n",
    "    v['label'] = np.stack((_label, 1-_label), axis=-1)\n",
    "    v['train_val_test'] = df['ttv'].values\n",
    "    v['n_parts'] = n_particles\n",
    "\n",
    "    del px, py, pz, energy, _px, _py, _pz, _e\n",
    "    del jet_px, jet_py, jet_pz, jet_energy, jet_p4, jet_pt, log_jet_pt, log_jet_energy, part_eta, eta_rel\n",
    "\n",
    "    return v"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGGkS6vzlGjY"
   },
   "source": [
    "def convert(source, destdir, basename, step=None, limit=None):\n",
    "    \"\"\"\n",
    "    Converts the DataFrame into an Awkward array and performs the read-write\n",
    "    operations for the same. Also performs Batching of the file into smaller\n",
    "    Awkward files.\n",
    "\n",
    "    :param source: str, The location to the H5 file with the dataframe\n",
    "    :param destdir: str, The location we need to write to\n",
    "    :param basename: str, Prefix for all the output file names\n",
    "    :param step: int, Number of rows per awkward file, None for all rows in 1 file\n",
    "    :param limit: int, Number of rows to read.\n",
    "    \"\"\"\n",
    "    df = pd.read_hdf(source, key='table')\n",
    "    logging.info('Total events: %s' % str(df.shape[0]))\n",
    "    if limit is not None:\n",
    "        df = df.iloc[0:limit]\n",
    "        logging.info('Restricting to the first %s events:' % str(df.shape[0]))\n",
    "    if step is None:\n",
    "        step = df.shape[0]\n",
    "\n",
    "    # Initialize an empty DataFrame to accumulate transformed data\n",
    "    accumulated_df = pd.DataFrame()\n",
    "\n",
    "    idx = 0\n",
    "    # Generate files as batches based on step size, only 1 batch is default\n",
    "    for start in range(0, df.shape[0], step):\n",
    "        if not os.path.exists(destdir):\n",
    "            os.makedirs(destdir)\n",
    "        output = os.path.join(destdir, '%s_%d.parquet'%(basename, idx))  # Changed to .parquet\n",
    "        logging.info(output)\n",
    "        if os.path.exists(output):\n",
    "            logging.warning('... file already exists: continue ...')\n",
    "            continue\n",
    "        v = _transform(df, start=start, stop=start+step)  # Convert Awkward array to pandas DataFrame\n",
    "        # Convert the ordered dictionary to a DataFrame\n",
    "        print(v)\n",
    "        batch_df = pd.DataFrame(v)\n",
    "\n",
    "        # Append the batch DataFrame to the accumulated DataFrame\n",
    "        accumulated_df = pd.concat([accumulated_df, batch_df], ignore_index=True)\n",
    "\n",
    "        # Write the batch DataFrame to a Parquet file\n",
    "        batch_df.to_parquet(output)\n",
    "        idx += 1\n",
    "\n",
    "    del batch_df, v, df\n",
    "    return accumulated_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtGFq9TplP0v"
   },
   "source": [
    "PROJECT_DIR = os.getcwd()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wS7FWLAWlSky"
   },
   "source": [
    "v = convert(source = os.path.join(PROJECT_DIR, 'train.h5'), destdir = os.path.join(PROJECT_DIR, 'converted'), basename = 'train-file', limit = 5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from typing import Callable, Optional\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from rich.table import Table\n",
    "from rich.highlighter import ReprHighlighter\n",
    "from rich import box\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def dict2table(input_dict: dict, num_cols: int = 4, title: Optional[str] = None) -> Table:\n",
    "    num_items = len(input_dict)\n",
    "    num_rows = math.ceil(num_items / num_cols)\n",
    "    col = 0\n",
    "    data = {}\n",
    "    keys = []\n",
    "    vals = []\n",
    "\n",
    "    for i, (key, val) in enumerate(input_dict.items()):\n",
    "        keys.append(f'{key}:')\n",
    "\n",
    "        vals.append(val)\n",
    "        if (i + 1) % num_rows == 0:\n",
    "            data[col] = keys\n",
    "            data[col+1] = vals\n",
    "            keys = []\n",
    "            vals = []\n",
    "            col += 2\n",
    "\n",
    "    data[col] = keys\n",
    "    data[col+1] = vals\n",
    "\n",
    "    highlighter = ReprHighlighter()\n",
    "    message = tabulate(data, tablefmt='plain')\n",
    "    table = Table(title=title, show_header=False, box=box.HORIZONTALS)\n",
    "    table.add_row(highlighter(message))\n",
    "    return table"
   ],
   "metadata": {
    "id": "-wLZAmBzDG5o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from rich.console import Console as RichConsole\n",
    "from rich.logging import RichHandler\n",
    "from rich.spinner import Spinner\n",
    "from rich.table import Table\n",
    "from rich.status import Status\n",
    "from rich.live import Live\n",
    "from rich._log_render import LogRender\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "class LogStatus(Status):\n",
    "    def __init__(self,\n",
    "        status,\n",
    "        console: RichConsole,\n",
    "        level: int = logging.INFO,\n",
    "        enabled: bool = True,\n",
    "        speed: float = 1.0,\n",
    "        refresh_per_second: float = 12.5,\n",
    "    ):\n",
    "        super().__init__(status,\n",
    "            console=console,\n",
    "            spinner='simpleDots',\n",
    "            speed=speed,\n",
    "            refresh_per_second=refresh_per_second\n",
    "        )\n",
    "\n",
    "        self.status = status\n",
    "        self.level = level\n",
    "        self.enabled = enabled\n",
    "        spinner = Spinner('simpleDots', style='status.spinner', speed=speed)\n",
    "        record = logging.LogRecord(name=None, level=level, pathname=None, lineno=None, msg=None, args=None, exc_info=None)\n",
    "        handler = RichHandler(console=console)\n",
    "        table = Table.grid()\n",
    "        table.add_row(self.status, spinner)\n",
    "\n",
    "        self._spinner = LogRender(show_level=True, time_format='[%X]')(\n",
    "            console=console,\n",
    "            level=handler.get_level_text(record),\n",
    "            renderables=[table]\n",
    "        )\n",
    "        self._live = Live(\n",
    "            self.renderable,\n",
    "            console=console,\n",
    "            refresh_per_second=refresh_per_second,\n",
    "            transient=True,\n",
    "        )\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.enabled:\n",
    "            self._start_time = time()\n",
    "            return super().__enter__()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.enabled:\n",
    "            super().__exit__(exc_type, exc_val, exc_tb)\n",
    "            self._end_time = time()\n",
    "            self.console.log(f'{self.status}...done in {self._end_time - self._start_time:.2f} s', level=self.level)"
   ],
   "metadata": {
    "id": "bX6T1jC6DSQ-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class RemoveIsolatedNodes(BaseTransform):\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        mask = data.y.new_zeros(data.num_nodes, dtype=bool)\n",
    "        mask[data.edge_index[0]] = True\n",
    "        mask[data.edge_index[1]] = True\n",
    "        data = data.subgraph(mask)\n",
    "        return data"
   ],
   "metadata": {
    "id": "sL6hurAcDXvW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class RemoveSelfLoops(BaseTransform):\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        if hasattr(data, 'edge_index') and data.edge_index is not None:\n",
    "            data.edge_index, _ = remove_self_loops(data.edge_index)\n",
    "        if hasattr(data, 'adj_t'):\n",
    "            data.adj_t = data.adj_t.remove_diag()\n",
    "        return data"
   ],
   "metadata": {
    "id": "p9MHwfz_DaBj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Iterable\n",
    "from rich.console import Group\n",
    "from rich.padding import Padding\n",
    "from rich.table import Column, Table\n",
    "from rich.progress import Progress, SpinnerColumn, BarColumn, TimeElapsedColumn, Task\n",
    "from rich.highlighter import ReprHighlighter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainerProgress(Progress):\n",
    "    def __init__(self,\n",
    "                 num_epochs: int,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "\n",
    "        progress_bar = [\n",
    "            SpinnerColumn(),\n",
    "            \"{task.description}\",\n",
    "            \"[cyan]{task.completed:>3}[/cyan]/[cyan]{task.total}[/cyan]\",\n",
    "            \"{task.fields[unit]}\",\n",
    "            BarColumn(),\n",
    "            \"[cyan]{task.percentage:>3.0f}[/cyan]%\",\n",
    "            TimeElapsedColumn(),\n",
    "            # \"{task.fields[metrics]}\"\n",
    "        ]\n",
    "\n",
    "        console = Console()\n",
    "\n",
    "        super().__init__(*progress_bar, console=console, **kwargs)\n",
    "\n",
    "        self.trainer_tasks = {\n",
    "            'epoch': self.add_task(total=num_epochs, metrics='', unit='epochs', description='overal progress'),\n",
    "            'train': self.add_task(metrics='', unit='steps', description='training', visible=False),\n",
    "            'val':   self.add_task(metrics='', unit='steps', description='validation', visible=False),\n",
    "            'test':  self.add_task(metrics='', unit='steps', description='testing', visible=False),\n",
    "        }\n",
    "\n",
    "        self.max_rows = 0\n",
    "\n",
    "    def update(self, task: Task, **kwargs):\n",
    "        if 'metrics' in kwargs:\n",
    "            kwargs['metrics'] = self.render_metrics(kwargs['metrics'])\n",
    "\n",
    "        super().update(self.trainer_tasks[task], **kwargs)\n",
    "\n",
    "    def reset(self, task: Task, **kwargs):\n",
    "        super().reset(self.trainer_tasks[task], **kwargs)\n",
    "\n",
    "    def render_metrics(self, metrics: Metrics) -> str:\n",
    "        out = []\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            metric_str = ' '.join(f'{k}: {v:.3f}' for k, v in metrics.items() if f'{split}/' in k)\n",
    "            out.append(metric_str)\n",
    "\n",
    "        return '  '.join(out)\n",
    "\n",
    "    def make_tasks_table(self, tasks: Iterable[Task]) -> Table:\n",
    "        \"\"\"Get a table to render the Progress display.\n",
    "\n",
    "        Args:\n",
    "            tasks (Iterable[Task]): An iterable of Task instances, one per row of the table.\n",
    "\n",
    "        Returns:\n",
    "            Table: A table instance.\n",
    "        \"\"\"\n",
    "        table_columns = (\n",
    "            (\n",
    "                Column(no_wrap=True)\n",
    "                if isinstance(_column, str)\n",
    "                else _column.get_table_column().copy()\n",
    "            )\n",
    "            for _column in self.columns\n",
    "        )\n",
    "\n",
    "        highlighter = ReprHighlighter()\n",
    "        table = Table.grid(*table_columns, padding=(0, 1), expand=self.expand)\n",
    "\n",
    "        if tasks:\n",
    "            epoch_task = tasks[0]\n",
    "            metrics = epoch_task.fields['metrics']\n",
    "\n",
    "            for task in tasks:\n",
    "                if task.visible:\n",
    "                    table.add_row(\n",
    "                        *(\n",
    "                            (\n",
    "                                column.format(task=task)\n",
    "                                if isinstance(column, str)\n",
    "                                else column(task)\n",
    "                            )\n",
    "                            for column in self.columns\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            self.max_rows = max(self.max_rows, table.row_count)\n",
    "            pad_top = 0 if epoch_task.finished else self.max_rows - table.row_count\n",
    "            group = Group(table, Padding(highlighter(metrics), pad=(pad_top,0,0,2)))\n",
    "            return Padding(group, pad=(0,0,1,18))\n",
    "\n",
    "        else:\n",
    "            return table"
   ],
   "metadata": {
    "id": "mVD_2vO2DrSd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch_geometric.data import Data\n",
    "from abc import ABC, abstractmethod\n",
    "from class_resolver.contrib.torch import optimizer_resolver\n",
    "\n",
    "# Defined as a subclass of both Module and ABC, inheriting attributes from both.\n",
    "\n",
    "# Serves as a base class for all the trainable modules in the project (baseline{i.e. without persistence}, Persistent Mod, etc.)\n",
    "class TrainableModule(Module, ABC):\n",
    "    def __init__(self, optimizer: str, learning_rate: float, weight_decay: float):\n",
    "        super().__init__()\n",
    "        self.optimizer_name = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args, **kwargs): pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, data: Data, phase: Phase) -> tuple[Optional[Tensor], Metrics]: pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, data: Data) -> Tensor: pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset_parameters(self): pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optimizer_resolver.make(\n",
    "            query=self.optimizer_name,\n",
    "            params=self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ],
   "metadata": {
    "id": "bnBc506rDyFV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from typing import Iterable, Optional, Annotated, Literal\n",
    "from torch.optim import Optimizer\n",
    "from torch.types import Number\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 monitor:       str = 'val/acc',\n",
    "                 monitor_mode:  Literal['min', 'max'] = 'max',\n",
    "                 epochs:        Annotated[int,  ArgInfo(help='number of epochs for training')] = 100,\n",
    "                 device:        Annotated[str,  ArgInfo(help='device to use for training', choices=['cpu', 'cuda', 'auto'])] = 'auto',\n",
    "                 verbose:       Annotated[bool, ArgInfo(help='display progress')] = True,\n",
    "                 logger:        Logger = None,\n",
    "                 ):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.monitor = monitor\n",
    "        self.monitor_mode = monitor_mode\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger or DummyLogger()\n",
    "\n",
    "        # setup device\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        # trainer internal state\n",
    "        self.model: TrainableModule = None\n",
    "        self.metrics: dict[str, MeanMetric] = {}\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.model = None\n",
    "        self.metrics.clear()\n",
    "\n",
    "    def update_metrics(self, metric_name: str, metric_value: object, batch_size: int = 1) -> None:\n",
    "        # if this is a new metric, add it to self.metrics\n",
    "        device = metric_value.device if torch.is_tensor(metric_value) else 'cpu'\n",
    "        if metric_name not in self.metrics:\n",
    "            self.metrics[metric_name] = MeanMetric().to(device)\n",
    "\n",
    "        # update the metric\n",
    "        self.metrics[metric_name].update(metric_value, weight=batch_size)\n",
    "\n",
    "    def aggregate_metrics(self, phase: Phase='train') -> Metrics:\n",
    "        metrics = {}\n",
    "\n",
    "        for metric_name, metric_value in self.metrics.items():\n",
    "            if phase in metric_name.split('/'):\n",
    "                value = metric_value.compute()\n",
    "                metric_value.reset()\n",
    "                metrics[metric_name] = value\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def is_better(self, current_metric: Number, previous_metric: Number) -> bool:\n",
    "        assert self.monitor_mode in ['min', 'max'], f'Unknown metric mode: {self.monitor_mode}'\n",
    "        if self.monitor_mode == 'max':\n",
    "            return current_metric > previous_metric\n",
    "        elif self.monitor_mode == 'min':\n",
    "            return current_metric < previous_metric\n",
    "\n",
    "    def fit(self,\n",
    "            model: TrainableModule,\n",
    "            train_dataloader: Iterable,\n",
    "            val_dataloader: Optional[Iterable]=None,\n",
    "            test_dataloader: Optional[Iterable]=None,\n",
    "            ) -> Metrics:\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer: Optimizer = self.model.configure_optimizers()\n",
    "\n",
    "        self.progress = TrainerProgress(\n",
    "            num_epochs=self.epochs,\n",
    "            disable=not self.verbose,\n",
    "        )\n",
    "\n",
    "        best_state_dict = None\n",
    "        best_metrics = None\n",
    "\n",
    "        with self.progress:\n",
    "            for epoch in range(1, self.epochs + 1):\n",
    "                metrics = {f'epoch': epoch}\n",
    "\n",
    "                # train loop\n",
    "                train_metrics = self.loop(train_dataloader, phase='train')\n",
    "                metrics.update(train_metrics)\n",
    "\n",
    "                # validation loop\n",
    "                if val_dataloader:\n",
    "                    val_metrics = self.loop(val_dataloader, phase='val')\n",
    "                    metrics.update(val_metrics)\n",
    "\n",
    "                    if best_metrics is None or self.is_better(\n",
    "                        metrics[self.monitor], best_metrics[self.monitor]\n",
    "                        ):\n",
    "                        best_metrics = metrics\n",
    "                        best_state_dict = deepcopy(self.model.state_dict())\n",
    "\n",
    "                # test loop\n",
    "                if test_dataloader:\n",
    "                    test_metrics = self.loop(test_dataloader, phase='test')\n",
    "                    metrics.update(test_metrics)\n",
    "\n",
    "                # log and update progress\n",
    "                self.progress.update(task='epoch', metrics=metrics, advance=1)\n",
    "                self.logger.log(metrics)\n",
    "\n",
    "        if best_metrics is None:\n",
    "            best_metrics = metrics\n",
    "        else:\n",
    "            self.model.load_state_dict(best_state_dict)\n",
    "\n",
    "        # log and return best metrics\n",
    "        self.logger.log_summary(best_metrics)\n",
    "\n",
    "        return best_metrics\n",
    "\n",
    "    def test(self, dataloader: Iterable) -> Metrics:\n",
    "        self.metrics.clear()\n",
    "        metrics = self.loop(dataloader, phase='test')\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, dataloader: Iterable, move_to_cpu: bool=False) -> Metrics:\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = self.to_device(batch)\n",
    "                # out might be a tuple of predictions\n",
    "                out = self.model.predict(batch)\n",
    "                if move_to_cpu:\n",
    "                    out = out.cpu()\n",
    "                preds.append(out)\n",
    "\n",
    "        # concatenate predictions, check if they are tuples\n",
    "        if isinstance(preds[0], tuple):\n",
    "            preds = tuple(torch.cat([p[i] for p in preds]) for i in range(len(preds[0])))\n",
    "        else:\n",
    "            preds = torch.cat(preds)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def loop(self, dataloader: Iterable, phase: Phase) -> Metrics:\n",
    "        self.model.train(phase == 'train')\n",
    "        grad_state = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(phase == 'train')\n",
    "        self.progress.update(phase, visible=len(dataloader) > 1, total=len(dataloader))\n",
    "\n",
    "        for batch in dataloader:\n",
    "            batch = self.to_device(batch)\n",
    "            metrics = self.step(batch, phase)\n",
    "            for item in metrics:\n",
    "                self.update_metrics(item, metrics[item], batch_size=batch.batch_nodes.size(0))\n",
    "            self.progress.update(phase, advance=1)\n",
    "\n",
    "        self.progress.reset(phase, visible=False)\n",
    "        torch.set_grad_enabled(grad_state)\n",
    "        return self.aggregate_metrics(phase)\n",
    "\n",
    "    def step(self, batch, phase: Phase) -> Metrics:\n",
    "        if phase == 'train':\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, metrics = self.model.step(batch, phase=phase)\n",
    "\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def to_device(self, batch):\n",
    "        if isinstance(batch, tuple):\n",
    "            return tuple(item.to(self.device) for item in batch)\n",
    "        return batch.to(self.device)"
   ],
   "metadata": {
    "id": "loCxB7-QED6B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn import Module, MultiheadAttention\n",
    "from torch_geometric.nn import JumpingKnowledge as JK, Linear\n",
    "\n",
    "\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    def forward(self, xs: Tensor) -> Tensor:\n",
    "        \"\"\"forward propagation\n",
    "\n",
    "        Args:\n",
    "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output tensor with size (num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = xs.transpose(2, int(self.batch_first))\n",
    "        out: Tensor = super().forward(x, x, x, need_weights=True)[0]\n",
    "        return out.mean(dim=int(self.batch_first))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super()._reset_parameters()\n",
    "\n",
    "\n",
    "class WeightedSum(Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = Linear(in_channels=hidden_dim, out_channels=num_heads, bias=False)\n",
    "\n",
    "        if num_heads > 1:\n",
    "            self.fc = Linear(in_channels=num_heads, out_channels=1, bias=False)\n",
    "\n",
    "    def forward(self, xs: Tensor) -> Tensor:\n",
    "        \"\"\"forward propagation\n",
    "\n",
    "        Args:\n",
    "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: output tensor with size (num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        H = xs.transpose(1, 2)  # (node, hop, dim)\n",
    "        W = self.Q(H).softmax(dim=1)  # (node, hop, head)\n",
    "        Z = H.transpose(1, 2).matmul(W)\n",
    "\n",
    "        if self.num_heads > 1:\n",
    "            Z = self.fc(Z)\n",
    "\n",
    "        return Z.squeeze(-1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Q.reset_parameters()\n",
    "        if self.num_heads > 1:\n",
    "            self.fc.reset_parameters()\n",
    "\n",
    "\n",
    "class JumpingKnowledge(Module):\n",
    "    supported_modes = ['cat', 'max', 'lstm', 'sum', 'mean', 'attn', 'wsum']\n",
    "    def __init__(self, mode: str, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        if mode == 'attn':\n",
    "            self.hidden_dim = kwargs['hidden_dim']\n",
    "            self.num_heads = kwargs['num_heads']\n",
    "            self.attn = SelfAttention(self.hidden_dim, num_heads=self.num_heads, batch_first=True)\n",
    "        elif mode == 'wsum':\n",
    "            self.hidden_dim = kwargs['hidden_dim']\n",
    "            self.num_heads = kwargs['num_heads']\n",
    "            self.wsum = WeightedSum(self.hidden_dim, num_heads=self.num_heads)\n",
    "        elif mode == 'lstm':\n",
    "            self.lstm = JK(mode='lstm', **kwargs)\n",
    "\n",
    "    def forward(self, xs: Tensor) -> Tensor:\n",
    "        \"\"\"forward propagation\n",
    "\n",
    "        Args:\n",
    "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: aggregated output with shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        if self.mode == 'cat':\n",
    "            return xs.transpose(1,2).reshape(xs.size(0), -1)\n",
    "        elif self.mode == 'sum':\n",
    "            return xs.sum(dim=-1)\n",
    "        elif self.mode == 'mean':\n",
    "            return xs.mean(dim=-1)\n",
    "        elif self.mode == 'max':\n",
    "            return xs.max(dim=-1)[0]\n",
    "        elif self.mode == 'attn':\n",
    "            return self.attn(xs)\n",
    "        elif self.mode == 'wsum':\n",
    "            return self.wsum(xs)\n",
    "        elif self.mode == 'lstm':\n",
    "            return self.lstm(xs.unbind(dim=-1))\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unsupported JK mode: {self.mode}')\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for module in self.children():\n",
    "            module.reset_parameters()"
   ],
   "metadata": {
    "id": "Ef1DQrWDEeWh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXR6S3e-mYnS"
   },
   "source": [
    "## Building the Persistent Homology Module\n",
    "\n",
    "`~`\n",
    "\n",
    "`Algebraic Topology` works to describe the shape of a `continuous manifold`. However, real-world datasets are typically given as point clouds, a discrete set of points sampled from an underlying manifold. In this setting, true homologies are trivial, as there is one connected component per point and no holes whatsoever; instead, `persistent homology` can be used to find holes in point clouds and to assign an importance score called persistence to each. Holes with high persistence are indicative of holes in the underlying manifold.\n",
    "\n",
    "* Persistent homology is a tool that computes topologially-informed features (or topological invariants) for a dataspace at different scales.\n",
    "\n",
    "* In simple terms, persistent homology keeps track of births and deaths of k-dimensional simplices. (vertices, edges, triangles, tetrahedra and so on)\n",
    "\n",
    "* This scale grows from local, and extends upto a global level. (upto infinity in theory)\n",
    "\n",
    "* Persistent features are computed for entities known as `Abstract Simplicial Complexes`\n",
    "\n",
    "* The simplicial complexes can be determined by one, or more parameters - `d`.\n",
    "\n",
    "* Here, varying the parameter(s) `d` in an increasing manner creates supersets of abstract simplicial complexes created before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUi8e2FUnkeO",
    "outputId": "103d9006-7553-4abe-83f3-54ff1765cec5"
   },
   "source": [
    "! pip install torch-geometric"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8J7wkerUF81"
   },
   "source": [
    "`Ripser` and `Persim` are some of the libraries that are based out of C++ and are used to compute filtrations and persistent homology of given point cloud data.\n",
    "\n",
    "Persistent homology can work pretty well for low-dimensional data, and our purpose of applying this idea here is to just obtain useful filtrations/globally informed features that can be further used to ease the task of GNN learning.\n",
    "\n",
    "If this approach is successful in our task of quark-gluon classification, it might be indicative of how different instances of particle jets might be just the outcome of sampling different low-dimensional manifolds in a higher-dimensional ambient space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDnsX-aUnktB",
    "outputId": "77516d2c-1f4e-4a11-c3e0-0930a3168fb3"
   },
   "source": [
    "! pip install ripser persim"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEzvyZ_ZV4c1"
   },
   "source": [
    "## Beyond \"d\" : Using Zig-Zag or Multiparameter Persistence\n",
    "\n",
    "Persistent homology is wel suited for detecting structure in high-dimensional datasets, so it is no surprise that the technique has mostly been applied in cosmology to either constrain non-Gaussianity in the CMB or find cosmic voids or filament loops in the large-scale structure of matter.\n",
    "<br>\n",
    "\n",
    "Never before, has persistent homology been used in particle physics."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install chart-studio"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZhwPgcJtZv6",
    "outputId": "67ce18a3-1ce5-4d22-c4cf-0c5f6766f994"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import chart_studio\n",
    "username='DarthRevan07'\n",
    "api_key='oUtdAdgKoP0P8GJpLWiP'\n",
    "chart_studio.tools.set_credentials_file(username=username,\n",
    "                                        api_key=api_key)"
   ],
   "metadata": {
    "id": "i7lAOF6EtjN-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import chart_studio.plotly as py\n",
    "import chart_studio.tools as tls\n",
    "import plotly.express as px"
   ],
   "metadata": {
    "id": "jU7dmtvyt3K8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbGtpsFYVzYF"
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i81DeD3JVzV9",
    "outputId": "99e09b67-fa73-4a63-b10d-caa52cdad231"
   },
   "source": [
    "! pip install --upgrade hepml"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gXpKJQWVzT4"
   },
   "source": [
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from hepml.core import download_dataset\n",
    "from scipy import ndimage\n",
    "\n",
    "# tda magic\n",
    "from gtda.homology import VietorisRipsPersistence, CubicalPersistence\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.plotting import plot_heatmap, plot_point_cloud, plot_diagram\n",
    "from gtda.pipeline import Pipeline\n",
    "from hepml.core import make_point_clouds, load_shapes\n",
    "from gtda.graphs import GraphGeodesicDistance\n",
    "\n",
    "# ml tools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# dataviz\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKMHNQA1amAL",
    "outputId": "b985a390-59b3-425b-d42e-ca6422fd988b"
   },
   "source": [
    "point_clouds_basic, labels_basic = make_point_clouds(n_samples_per_shape=100, n_points=20, noise=0.5)\n",
    "point_clouds_basic.shape, labels_basic.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "type(point_clouds_basic[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgLmOAt_v9TT",
    "outputId": "77e1e69d-faa7-42ae-ec67-ee81b5cf1001"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig = px.scatter_3d(point_clouds_basic[0])\n",
    "fig.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Dr5wGqM1u_Sr",
    "outputId": "3647667b-0211-4601-9a88-5f7f2cc91151"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "py.plot(fig, filename=\"plotly_scatter\", auto_open = True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BKcKEgoduWJM",
    "outputId": "1273c110-b0af-451b-816e-40480f93ae58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "oXlaYPz5chs4",
    "outputId": "4c1a2ba5-b358-403d-f981-dddd5a2b4157"
   },
   "source": [
    "plot_point_cloud(point_clouds_basic[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "4l-q--NCcwvW",
    "outputId": "676d10b9-c366-451a-ecdb-83a0dd2f8088"
   },
   "source": [
    "plot_point_cloud(point_clouds_basic[100])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "pqI8s7FVcxI3",
    "outputId": "03927fe7-f1e0-4286-8213-6f9847bb76bf"
   },
   "source": [
    "plot_point_cloud(point_clouds_basic[-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYVKCxGnVzNJ"
   },
   "source": [
    "from gtda.point_clouds import ConsistentRescaling, ConsecutiveRescaling\n",
    "from gtda.graphs import TransitionGraph, KNeighborsGraph\n",
    "\n",
    "def adjust_density(point_cloud, density_factor):\n",
    "    \"\"\"Adjust point cloud density.\"\"\"\n",
    "    return point_cloud * density_factor\n",
    "\n",
    "def adjust_distance(point_cloud, *args):\n",
    "    \"\"\"Adjust point cloud distances.\"\"\"\n",
    "    cr = ConsistentRescaling()\n",
    "    point_cloud = cr.fit_transform(point_cloud, *args)\n",
    "    return point_cloud"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKggrfyHVzKr"
   },
   "source": [
    "from gtda.diagrams import PersistenceLandscape\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import networkx as nx\n",
    "from gtda.plotting import plot_diagram, plot_betti_curves\n",
    "\n",
    "class VietorisPersistenceModule:\n",
    "  def __init__(self, max_edge_length = np.inf, homology_dim = (0, 1, 2, 3)):\n",
    "    self.point_clouds_basic, self.labels_basic = make_point_clouds(n_samples_per_shape=100, n_points=40, noise=0.5)\n",
    "    self.point_cloud = self.point_clouds_basic[0]\n",
    "    self.max_edge_length = max_edge_length\n",
    "    self.homology_dim = homology_dim\n",
    "    self.persistence_diagram = None\n",
    "    self.betti_numbers = None\n",
    "    self.adj_graph = None\n",
    "    self.rips_complex = None\n",
    "\n",
    "  def vietoris_rips_complex(self):\n",
    "    self.rips_complex = VietorisRipsPersistence(metric = 'euclidean',\n",
    "                               max_edge_length = self.max_edge_length,\n",
    "                               homology_dimensions = self.homology_dim)\n",
    "\n",
    "    self.persistence_diagram = self.rips_complex.fit_transform([self.point_cloud])[0]\n",
    "\n",
    "\n",
    "  def compute_betti(self):\n",
    "        landscape = PersistenceLandscape(n_layers=1, n_bins=100, n_jobs=6)\n",
    "        landscapes = landscape.fit_transform([self.persistence_diagram])\n",
    "\n",
    "        # Compute Betti numbers\n",
    "        self.betti_numbers = [\n",
    "            np.sum(landscape[i]) for landscape, i in zip(landscapes, self.homology_dim)\n",
    "        ]\n",
    "\n",
    "        # Plot Betti curves\n",
    "        fig = plot_betti_curves(\n",
    "            landscapes[0],\n",
    "            samplings=landscape.sampling_range_[0],\n",
    "            homology_dimensions=self.homology_dim\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "  def plot_persistence_diagram(self):\n",
    "        # Plot the persistence diagram\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        plot_diagram(self.persistence_diagram, ax=ax, show=False)\n",
    "        plt.title(\"Persistence Diagram\")\n",
    "        plt.show()\n",
    "\n",
    "  def create_persistent_graph(self):\n",
    "      # Get the adjacency matrix from the persistence module (note: fit_transform returns the diagram)\n",
    "      adjacency_matrix = self.rips_complex.fit_transform(self.point_cloud.reshape(1, *self.point_cloud.shape))\n",
    "\n",
    "      # Convert the persistence diagram to a graph structure\n",
    "      self.adj_graph = nx.Graph()\n",
    "\n",
    "      # Here, we extract edges from the adjacency matrix that correspond to a certain filtration value\n",
    "      for i in range(len(self.point_cloud)):\n",
    "          for j in range(i+1, len(self.point_cloud)):\n",
    "              if adjacency_matrix[0][i][j] < self.max_edge_length:\n",
    "                  self.adj_graph.add_edge(i, j, weight=adjacency_matrix[0][i][j])\n",
    "\n",
    "      # Optionally, visualize the graph\n",
    "      pos = {i: self.point_cloud[i] for i in range(len(self.point_cloud))}\n",
    "      nx.draw(self.adj_graph, pos, with_labels=True, node_size=50)\n",
    "      plt.title(\"Graph from Vietoris-Rips Complex\")\n",
    "      plt.show()\n",
    "\n",
    "  def preprocess(self):\n",
    "    self.vietoris_rips_complex()\n",
    "    self.compute_betti()\n",
    "    self.plot_persistence_diagram()\n",
    "    self.create_persistent_graph()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8NM1yQdwxLt5",
    "outputId": "80ac4461-85bd-4912-d658-ab67954def00"
   },
   "source": [
    "\n",
    "# Initialize and run the analysis\n",
    "topology_analyzer = VietorisPersistenceModule(max_edge_length=1.5)\n",
    "topology_analyzer.preprocess()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLRlwBqPp0Nc"
   },
   "source": [
    "### Approaching Multiparameter Persistence - `RIVET`\n",
    "\n",
    "* Simply using a single parameter such as distance `d` will simply result in a k-Nearest Neighbor situation all over again, where the cost of applying persistent homology will become too significant and applying it would be impractical.\n",
    "\n",
    "* I would like to add another parameter that might be indicative of the presence of a global phenomenon in the vicinity of the particles present in a jet.\n",
    "\n",
    "* I have implemented a persistent module for 2 features here (distance and density for now).\n",
    "\n",
    "\n",
    "For this purpose, RIVET is a tool for topological data analysis, and more specifically, for the visualization and analysis of two-parameter persistent homology.\n",
    "\n",
    "A python API for RIVET is provided as `pyrivet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4v1QO5XYpz5K",
    "outputId": "3b1b044d-5a1c-4a79-9a70-57683b4532f7"
   },
   "source": [
    "! pip install pyrivet"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLE-jt2Io2wS"
   },
   "source": [
    "import numpy as np\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaA56DQHo_ro"
   },
   "source": [
    "class PersistentHomologyFeatureExtractor:\n",
    "    def __init__(self, data_entry):\n",
    "        self.data_entry = data_entry\n",
    "\n",
    "    def compute_persistence_diagram(self):\n",
    "        # Compute persistence diagram for the single data entry\n",
    "        diagram = ripser(self.data_entry.numpy(), maxdim=2)['dgms']\n",
    "        return diagram\n",
    "\n",
    "    def create_graph_from_diagram(self, diagram):\n",
    "        # Create a PyG Data object from the persistence diagram\n",
    "        # Here, we simplify the process by assuming nodes represent features\n",
    "        # and edges are determined by some criterion (e.g., proximity in filtration values)\n",
    "\n",
    "        # Example: Using the 0-dimension persistence diagram to create nodes\n",
    "        node_features = torch.tensor([feature for feature in diagram[0]], dtype=torch.float)\n",
    "        edge_indices = torch.tensor([[i, j] for i in range(len(diagram[0])) for j in range(i+1, len(diagram[0]))], dtype=torch.long)\n",
    "\n",
    "        # Create a PyG Data object\n",
    "        graph = Data(x=node_features, edge_index=edge_indices)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def preprocess(self):\n",
    "        persistence_diagram = self.compute_persistence_diagram()\n",
    "        graph_object = self.create_graph_from_diagram(persistence_diagram)\n",
    "        return graph_object\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqnNHMDtTIdB",
    "outputId": "c921fa86-54c4-4457-9e65-ef0ba4fa0e41"
   },
   "source": [
    "# Generate a random dataset with 1 entry having 140 points with 4 features\n",
    "data_entry = torch.tensor(np.random.rand(140, 4), dtype=torch.float)\n",
    "\n",
    "# Instantiate the PersistentHomologyFeatureExtractor with the generated data entry\n",
    "extractor = PersistentHomologyFeatureExtractor(data_entry)\n",
    "\n",
    "# Preprocess the data entry to compute persistent homology features and create a graph object\n",
    "graph_object = extractor.preprocess()\n",
    "\n",
    "# Print information about the generated graph object\n",
    "print(f\"Graph Object:\")\n",
    "print(f\"Node Features Shape: {graph_object.x.shape}\")\n",
    "print(f\"Edge Indices Shape: {graph_object.edge_index.shape}\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv5kdeFxTZct"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
